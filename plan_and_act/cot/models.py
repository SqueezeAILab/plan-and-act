import asyncio
import contextlib
import os
import traceback
from dataclasses import Field
from enum import Enum
from typing import Awaitable, Callable, Generic, Literal, TypeVar, cast

import openai
import tiktoken
from browser_env.actions import Action
from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast
from typing_extensions import TypedDict

##################################################################
#                 WebArena Lite Training Models                  #
##################################################################


class WebArenaLiteWebsite(Enum):
    SHOPPING_ADMIN = "shopping_admin"
    MAP = "map"
    SHOPPING = "shopping"
    REDDIT = "reddit"
    GITLAB = "gitlab"


class Conversation(TypedDict):
    """
    The original WebArena Lite data is represented as a list of conversations.
    Each conversation has a from_ field which can be "gpt" or "human". The value field is the content of the message. "gpt" message contains the HTML (and the task query) as well as the previous actions. "human" message contains the next immediate action.
    """

    from_: str
    value: str


class BaseData(TypedDict):
    """
    The base data that is shared by all the models.
    """

    task: str


class WebArenaLiteAction(BaseData):
    """
    This is one action in the WebArena Lite data. It's index_within_task is the index of the action in the task, and the conversations contains the "gpt" message (which contains the state and the task query) and the "human" message (which contains the next immediate action).

    Note: This is called the `WebArenaLiteAction` because the WebArena environment already has an `Action` class.
    """

    index_within_task: int
    conversations: list[Conversation]


class ProcessedData(BaseData):
    """
    The processed data that is used internally. This data groups the WebArena Lite data by task.
    Each task has a list of actions and the initial html state for convenient access.

    Overall, here is the structure of the data:
        - task: The task description
        - initial_html_state: The initial html state
        - actions: A list of actions
            - index_within_task: The index of the action in the task
            - conversations: A list of conversations (contains the HTML and the action.)
                - from_: The role of the conversation. Can be "gpt" or "human"
                - value: The content of the conversation

    This data is constructed from the original WebArena Lite data using the `preprocess_webarena_data` function in the `utils.py` file.
    """

    initial_html_state: str
    actions: list[WebArenaLiteAction]


##################################################
#                 Plan Data Models               #
##################################################


class Plan(TypedDict):
    """
    In CoT, plan is just a reasoning followed by the final plan. Both are free form text so that the LLM can reason and plan in any way it wants using its innate capabilities.
    """

    reasoning: str
    plan: str


class ActionStep(BaseData):
    """
    Before generating the final plan, we ask the LLM to generate a description for an action step by giving the HTML state before and after the action. This model is used to store the output of the LLM for each action step.
    """

    action: WebArenaLiteAction
    reasoning: str
    action_description: str


class PlanAnnotatorOutput(BaseData):
    """
    This is the final data that is used for training the plan model. A plan annotator LLM generates this piece of data.
    The LLM first generates the plan and then also generates the reasoning for it. This model stores the output of the LLM, both the first plan generation and the reasoning generation steps.
    """

    plan_generation_reasoning: str
    plan: str
    reasoning: str


class PlanTrainingData(BaseData):
    """
    This is the data that is used for training the plan model. It contains everything we need to construct the training data for the plan model. Check out the `utils.py` file for how to generate this into appropriate data format like torchtune or LLamaFactory formats.
    """

    task: str
    initial_html_state: str
    plan: Plan


class CotPlanAnnotatorDataGeneratorData(TypedDict):
    """
    This is how the data generated by the CotPlanAnnotatorDataGenerator is stored in the disk.
    """

    task: str
    data: tuple[list[ActionStep], PlanAnnotatorOutput, PlanTrainingData]


class CotSyntheticPlanDataGeneratorData(TypedDict):
    """
    This is how the data generated by the CotSyntheticPlanDataGenerator is stored in the disk.
    """

    # This is the list of in-context examples that were used to generate this particular synthetic plans
    in_context_examples: list[str]

    # This is the list of synthetic plans that were generated
    datas: list[PlanTrainingData]


##############################################
#            Executor Data Models            #
##############################################


class ExecutorAction(TypedDict):
    """
    In CoT, the executor LLM takes the plan, the current state of the task, and then generates a reasoning for it. This model stores the output of the LLM for the executor step as well as the action label and the next action. This is used for only for data annotation though.
    """

    reasoning: str
    action: WebArenaLiteAction


class ExecutorTrainingData(BaseData):
    """
    This is the data that is used for training the executor model. It contains everything we need to construct the training data for the executor model. Check out the `utils.py` file for how to generate this into appropriate data format like torchtune or LLamaFactory formats.
    """

    initial_html_state: str
    plan: Plan
    executor_actions: list[ExecutorAction]


class CotActAnnotatorDataGeneratorData(TypedDict):
    """
    This is how the data generated by the CotActAnnotatorDataGenerator is stored in the disk.
    """

    task: str
    data: ExecutorTrainingData


##################################################
#            Dynamic Replanning Models            #
##################################################

#
# Planner
#


class ReplanDecision(TypedDict):
    """
    Captures the decision about whether to replan and why.

    This data structure is used by the dynamic planner to record whether a replan
    is needed at a particular step and the detailed reasoning behind this decision.
    """

    needs_replan: bool
    reasoning: str


class ReplanStep(TypedDict):
    """
    Represents a single replan step in the dynamic replanning process.

    Each step contains:
    - index_within_task: Which action step this replan corresponds to
    - plan: The plan that should be followed at this step (may be unchanged from previous step)
    - html_state: The HTML state that was evaluated for replanning
    - previous_executor_actions: List of actions taken before this step
    - replan_decision: The decision about whether to replan and its reasoning
    """

    index_within_task: int
    plan: Plan
    html_state: str
    previous_executor_actions: list[WebArenaLiteAction]
    replan_decision: ReplanDecision


class PlanHistory(TypedDict):
    """
    History of plans and replanning decisions for a task.

    Contains:
    - initial_plan: The first plan generated for the task
    - replan_steps: A list of all replanning steps, including both actual replans
      and "no replan needed" decisions
    """

    initial_plan: Plan
    initial_html_state: str
    replan_steps: list[ReplanStep]


class DynamicPlanTrainingData(BaseData):
    """
    Training data for the dynamic planner model.

    This extends the standard plan training data to include the full history
    of plans and replanning decisions, enabling the model to learn when and how to replan.
    """

    initial_html_state: str
    plan_history: PlanHistory


class DynamicPlanAnnotatorDataGeneratorData(TypedDict):
    """
    Data generated by the DynamicCoTPlanAnnotator.

    Similar to CotPlanAnnotatorDataGeneratorData but stores the complete
    plan history rather than just a single plan.
    """

    task: str
    data: PlanHistory


class DynamicExecutorTrainingData(BaseData):
    """
    Training data for the replan-aware executor model.

    This extends the standard executor training data to include the full
    plan history, enabling the model to learn how to adapt to changing plans.
    """

    initial_html_state: str
    plan_history: PlanHistory
    executor_actions: list[ExecutorAction]


class DynamicActAnnotatorDataGeneratorData(TypedDict):
    """
    Data generated by the DynamicCoTActAnnotator.

    Similar to CotActAnnotatorDataGeneratorData but includes the complete
    plan history instead of just a single plan.
    """

    task: str
    data: DynamicExecutorTrainingData


###################################################
#            Inference Models                  #
###################################################


class PlanInferenceInput(TypedDict):
    """
    This is the input for the Planner model during evaluation.
    """

    task: str
    initial_html_state: str


class ActInferenceOutput(TypedDict):
    """
    This is the output for the Executor model during evaluation.
    """

    action_str: str
    reasoning: str


class ActInferencePreviousRound(TypedDict):
    """
    We need the HTML and the action output of an individual round in the action history.
    """

    act: ActInferenceOutput
    uncleaned_html: str


class ActInferenceInput(TypedDict):
    """
    This is the input for the Executor model during evaluation.
    """

    task: str
    plan: Plan
    previous_rounds: list[ActInferencePreviousRound] | list[ExecutorAction]
    current_round: ActInferencePreviousRound | ExecutorAction


class DynamicPlanInferenceInput(TypedDict):
    task: str
    previous_plan: Plan
    current_html_state: str
    previous_actions: list[WebArenaLiteAction] | list[ActInferenceOutput]


class DynamicExecutorInferenceInput(TypedDict):
    """
    Input for the dynamic executor during inference.
    Similar to ActInferenceInput but uses a PlanHistory instead of a single Plan.
    """

    task: str
    plan_history: PlanHistory
    previous_rounds: list[ActInferencePreviousRound] | list[ExecutorAction]
    current_round: ActInferencePreviousRound | ExecutorAction


###################################################
#            Plan-And-Act Models                  #
###################################################


class PlanAndActAction(TypedDict):
    """
    This is the action that is used for Plan-and-Act project. It contains the action object from the original WebArenaLite codebase as well as some useful information for Plan-and-Act project.
    """

    action: Action
    comment: str


class ServerMessage(TypedDict):
    role: Literal["system", "user", "assistant"]
    content: str


class ContextLengthExceededError(Exception):
    pass


class LLM:
    _max_length: int
    _max_tokens: int
    _model_name: str
    _tokenizer: tiktoken.Encoding | PreTrainedTokenizer | PreTrainedTokenizerFast
    _base_url: str | None
    _temperature: float

    def __init__(
        self,
        max_length: int,
        max_tokens: int,
        model_name: str,
        base_url: str | None = None,
        temperature: float = 0.0,
    ) -> None:
        self._max_length = max_length
        self._max_tokens = max_tokens
        self._model_name = model_name
        self._tokenizer = (
            tiktoken.encoding_for_model("gpt-4-turbo")
            if "gpt-4" in model_name
            else AutoTokenizer.from_pretrained(model_name)
        )
        self._base_url = base_url
        self._temperature = temperature

    async def acall_model(self, prompt: list[ServerMessage]) -> str | None:
        # Need to trim the prompt to fit the max length
        encoded_prompt = self._tokenizer.encode("".join([m["content"] for m in prompt]))
        if len(encoded_prompt) > self._max_length:
            raise ContextLengthExceededError()

        client = cast(openai.AsyncOpenAI, self.get_client(async_=True))

        completion = await client.chat.completions.create(
            model=self._model_name,
            messages=prompt,  # type: ignore
            temperature=self._temperature,
            max_tokens=self._max_tokens,
            timeout=9999999999999,
        )

        return completion.choices[0].message.content

    async def acall_completions(self, prompt: str) -> str | None:
        client = cast(openai.AsyncOpenAI, self.get_client(async_=True))

        completion = await client.completions.create(
            model=self._model_name,
            prompt=prompt,
            temperature=self._temperature,
            max_tokens=self._max_tokens,
            timeout=9999999999999,
        )

        return completion.choices[0].text

    def call_model(self, prompt: list[ServerMessage]) -> str | None:
        # Need to trim the prompt to fit the max length
        encoded_prompt = self._tokenizer.encode("".join([m["content"] for m in prompt]))
        if len(encoded_prompt) > self._max_length:
            raise ContextLengthExceededError()

        client = cast(openai.OpenAI, self.get_client(async_=False))

        completion = client.chat.completions.create(
            model=self._model_name,
            messages=prompt,  # type: ignore
            temperature=self._temperature,
            max_tokens=self._max_tokens,
        )

        return completion.choices[0].message.content

    def call_completions(self, prompt: str) -> str | None:
        client = cast(openai.OpenAI, self.get_client(async_=False))

        completion = client.completions.create(
            model=self._model_name,
            prompt=prompt,
        )

        return completion.choices[0].text

    def get_client(self, async_: bool) -> openai.OpenAI | openai.AsyncOpenAI:
        if "gpt-4" in self._model_name:
            class_ = openai.AsyncAzureOpenAI if async_ else openai.AzureOpenAI
            client = class_(
                azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
                api_key=os.environ["AZURE_OPENAI_API_KEY"],
                api_version=os.environ["AZURE_OPENAI_API_VERSION"],
            )
        elif self._base_url is not None and "together" in self._base_url:
            class_ = openai.AsyncOpenAI if async_ else openai.OpenAI
            client = class_(
                base_url=self._base_url,
                api_key=os.environ["TOGETHER_API_KEY"],
            )
        elif self._base_url is not None and "deepseek" in self._base_url:
            class_ = openai.AsyncOpenAI if async_ else openai.OpenAI
            client = class_(
                base_url=self._base_url,
                api_key=os.environ["DEEPSEEK_API_KEY"],
            )
        else:
            # Else this is an open source model
            class_ = openai.AsyncOpenAI if async_ else openai.OpenAI
            client = class_(
                base_url=self._base_url or "http://localhost:8000/v1",
                api_key="webarena",
            )

        return client


T = TypeVar("T")  # The type of each item in the input
R = TypeVar("R")  # The type of the result of processing each item
I = TypeVar("I")  # The type of the identity of the result


class AsyncDataGenerationJobEngine(Generic[T, R, I]):
    """
    A generic async data-generation job engine that processes items of type T,
    aggregates results of type R, and saves new results efficiently.
    """

    def __init__(
        self,
        data_to_process: list[T],
        task_fn: Callable[[T], Awaitable[R | list[R] | None]],
        save_fn: Callable[[list[R]], None] | None = None,
        identity_fn: Callable[[R], I] | None = None,
        already_processed: set[I] | None = None,
        concurrency: int = 3,
        save_interval: float = 60.0,
        progress_interval: float = 5.0,
    ) -> None:
        """
        :param data_to_process: List of data items to process.
        :param task_fn: An async function that takes an item of type T and returns an R (or list of R, or None).
        :param save_fn: An optional function that accepts a list of new results (of type R) and persists them,
                        e.g., by appending to a file.
        :param identity_fn: Optional function to derive a unique identity from a result.
        :param already_processed: Optional set of already processed identities to avoid duplication.
        :param concurrency: Number of worker coroutines to run concurrently.
        :param save_interval: Time interval (seconds) between periodic saves.
        :param progress_interval: Time interval (seconds) between progress updates.
        """
        self._data_to_process = data_to_process
        self._task_fn = task_fn
        self._save_fn = save_fn
        self._concurrency = concurrency
        self._save_interval = save_interval
        self._progress_interval = progress_interval
        self._identity_fn = identity_fn
        self._already_processed = already_processed or set()

        # Instead of accumulating everything in one big list,
        # we use an unsaved buffer to store only new results.
        self._unsaved_results: list[R] = []

        # Queues for internal processing and progress tracking
        self._queue: asyncio.Queue[T | None] = asyncio.Queue()
        self._completed: asyncio.Queue[None] = asyncio.Queue()

    async def _create_task_queue(self) -> None:
        """Populates the queue with items to process."""
        for data in self._data_to_process:
            await self._queue.put(data)

    async def _worker(self) -> None:
        """Worker that processes items and stores new results in the unsaved buffer."""
        while True:
            item = await self._queue.get()
            if item is None:
                self._queue.task_done()
                break

            try:
                result = await self._task_fn(item)
                if result is not None:
                    # Process both single results and lists of results.
                    if isinstance(result, list):
                        for r in result:
                            if self._identity_fn is not None:
                                identity = self._identity_fn(r)
                                if identity not in self._already_processed:
                                    self._unsaved_results.append(r)
                                    self._already_processed.add(identity)
                                else:
                                    # If already processed, optionally requeue the item.
                                    await self._queue.put(item)
                            else:
                                self._unsaved_results.append(r)
                    else:
                        if self._identity_fn is not None:
                            identity = self._identity_fn(result)
                            if identity not in self._already_processed:
                                self._unsaved_results.append(result)
                                self._already_processed.add(identity)
                            else:
                                await self._queue.put(item)
                        else:
                            self._unsaved_results.append(result)
                # Signal completion of one item.
                await self._completed.put(None)
            except Exception as e:
                print(f"Worker error: {e}")
                # Prin the stack trace as well
                print(traceback.format_exc())
            finally:
                self._queue.task_done()

    async def _periodic_saver(self) -> None:
        """
        Periodically calls save_fn to persist only the unsaved results.
        The save_fn should be implemented to append new results to persistent storage.
        """
        try:
            while True:
                await asyncio.sleep(self._save_interval)
                if self._save_fn is not None:
                    # Copy and clear the unsaved results buffer
                    new_results = self._unsaved_results[:]
                    self._unsaved_results.clear()
                    if new_results:
                        self._save_fn(new_results)
                        print(f"Periodic saver: Saved {len(new_results)} new results.")
                    else:
                        print("Periodic saver: No new results to save.")
                else:
                    print("Periodic saver: No save_fn defined; skipping save.")
        except asyncio.CancelledError:
            # Final save on cancellation
            if self._save_fn is not None:
                new_results = self._unsaved_results[:]
                self._unsaved_results.clear()
                if new_results:
                    self._save_fn(new_results)
                    print(
                        f"Periodic saver: Final save of {len(new_results)} new results on cancellation."
                    )
            raise

    async def _progress_monitor(self, total_tasks: int) -> None:
        """Prints progress at regular intervals."""
        completed_count = 0
        try:
            while True:
                await asyncio.sleep(self._progress_interval)
                tasks_done = 0
                while not self._completed.empty():
                    await self._completed.get()
                    tasks_done += 1
                completed_count += tasks_done
                print(f"Progress: Completed {completed_count}/{total_tasks} tasks.")
        except asyncio.CancelledError:
            # Final progress update
            tasks_done = 0
            while not self._completed.empty():
                await self._completed.get()
                tasks_done += 1
            completed_count += tasks_done
            print(
                f"Progress monitor stopping. Final count: {completed_count}/{total_tasks} tasks completed."
            )
            raise

    async def run(self) -> None:
        """
        Runs the job engine:
          1. Populate the task queue.
          2. Start worker tasks.
          3. Launch periodic saver and progress monitor.
          4. Wait for processing completion.
          5. Cancel saver/monitor and do a final save.
        """
        # 1. Populate the queue
        await self._create_task_queue()
        total_tasks = self._queue.qsize()
        print(f"Processing {total_tasks} tasks with {self._concurrency} workers...")

        # 2. Create worker tasks
        workers = [
            asyncio.create_task(self._worker()) for _ in range(self._concurrency)
        ]

        # 3. Start the periodic saver and progress monitor tasks
        saver_task = asyncio.create_task(self._periodic_saver())
        monitor_task = asyncio.create_task(self._progress_monitor(total_tasks))

        # 4. Wait for all tasks in the queue to be processed
        await self._queue.join()

        # Signal workers to exit by adding one None per worker
        for _ in workers:
            await self._queue.put(None)
        await asyncio.gather(*workers)

        # 5. Cancel the saver and monitor tasks, with a final save
        saver_task.cancel()
        monitor_task.cancel()
        with contextlib.suppress(asyncio.CancelledError):
            await saver_task
        with contextlib.suppress(asyncio.CancelledError):
            await monitor_task

        # Final save of any remaining unsaved results
        if self._save_fn is not None:
            new_results = self._unsaved_results[:]
            self._unsaved_results.clear()
            if new_results:
                self._save_fn(new_results)
                print(
                    f"All tasks processed. Final save: {len(new_results)} new results saved."
                )
            else:
                print("All tasks processed. No new results to save.")
        else:
            print("All tasks processed. No final save function defined.")


####################################################
#            Training Data Formats                 #
####################################################


class TorchTuneData(TypedDict):
    input: str
    output: str


class LlamaFactoryData(TypedDict):
    instruction: str
    input: Literal[""]  # This is a frozen constant of empty string
    output: str
